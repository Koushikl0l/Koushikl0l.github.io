<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Robust Fusion and Contrastive Learning for Multimodal AQI Regression</title>
  <style>
    body {
      font-family: "Times New Roman", Times, serif;
      font-size: 14px;
      margin: 0;
      padding: 0;
      background-color: #fff;
      color: #000;
    }

    .container {
      width: 90%;
      max-width: 1100px;
      margin: 40px auto;
      border: 1.5px solid #ccc;
      border-radius: 12px;
      padding: 30px;
      box-sizing: border-box;
    }

    .title {
      text-align: center;
      font-size: 22px;
      font-weight: bold;
    }

    .author {
      text-align: center;
      font-size: 14px;
      margin-bottom: 20px;
    }

    .abstract-box {
      border: 1px dashed #999;
      padding: 15px;
      margin-bottom: 25px;
    }

    .columns {
      display: flex;
      gap: 20px;
      align-items: flex-start;
    }

    .column {
      flex: 1;
    }

    .section {
      margin-bottom: 20px;
    }

    .dashed-box {
      border: 1.5px dashed #666;
      padding: 10px;
      margin-bottom: 20px;
      background-color: #fcfcfc;
    }

    .keyword-section {
      width: 93%;
      padding: 10px 15px;
      font-size: 13.5px;
      margin-bottom: 20px;
      background-color: #fdfdfd;
    }

    .keyword-section h2 {
      margin-top: 0;
      font-size: 15px;
      text-align: left;
    }

    .keyword-section ul {
      list-style-type: none;
      padding-left: 0;
      line-height: 1.5;
      margin: 0;
    }

    .keyword-section li {
      margin-bottom: 6px;
    }

    .figure {
      text-align: center;
      font-size: 12px;
      margin-top: 10px;
      margin-bottom: 20px;
    }

    .image-grid {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 10px;
      margin: 20px 0;
    }

    .image-grid img {
      width: 100%;
      border: 1px solid #999;
      border-radius: 4px;
    }

    .caption {
      text-align: center;
      font-size: 11px;
      color: #444;
    }

    h2 {
      font-size: 16px;
      margin: 15px 0 5px;
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="title">
      Robust Fusion and Contrastive Learning for Multimodal AQI Regression
    </div>
    <div class="author">
      Koushik Ahmed Kushal <br>
      Department of Computer Science, American International University-Bangladesh <br>
      koushikahmedkushal@gmail.com
    </div>

    <div class="abstract-box">
      <strong>ABSTRACT</strong>
      <!--
      <p>
        A multimodal approach for predicting Air Quality Index (AQI) by integrating numerical and image data.
        A precision-integrated numerical branch and an image encoder using EfficientNetB0 extract features.
        Cross-modal attention is utilized to enhance interaction between modalities, followed by multi-head self-attention
        and a gated fusion mechanism for robust integration. A contrastive loss function is incorporated during training
        to align numerical and image embeddings, leading to state-of-the-art performance in AQI regression from multiple data sources.
      </p>
      -->
      <p>
        A multimodal approach for predicting Air Quality Index (AQI) by integrating numerical and image data<sup><a href="#ref1">[1]</a></sup>.
        A precision-integrated numerical branch and an image encoder using EfficientNetB0 extract features<sup><a href="#ref2">[2]</a></sup>.
        Cross-modal attention is utilized to enhance interaction between modalities, followed by multi-head self-attention
        and a gated fusion mechanism for robust integration<sup><a href="#ref4">[4]</a></sup>. A contrastive loss function is incorporated during training
        to align numerical and image embeddings<sup><a href="#ref5">[5]</a></sup>, leading to state-of-the-art performance in AQI regression from multiple data sources<sup><a href="#ref1">[1]</a></sup>.
      </p>
    </div>

    <div class="columns">
      <!-- Left Column -->
      <div class="column">
        <!-- Keywords box -->
        <div class="section dashed-box keyword-section" >
          <h2>KEYWORDS</h2>
          <ul>
            <li><strong>Multimodal Learning</strong> – Integration of image and sensor data.</li>
            <li><strong>AQI Regression</strong> – Predicting AQI as a continuous variable.</li>
            <li><strong>Contrastive Learning</strong> – Aligning multi-modal embeddings.</li>
            <li><strong>Cross-Modal Attention</strong> – Sensor-data attending to image features.</li>
            <li><strong>Gated Fusion</strong> – Dynamic modality balancing.</li>
            <li><strong>EfficientNetB0</strong> – Lightweight CNN for feature extraction.</li>
            <li><strong>Environmental Monitoring</strong> – Urban air quality analysis.</li>
          </ul>
        </div>

        <!-- Introduction -->
        <div class="section dashed-box">
          <h2>INTRODUCTION</h2>
          <p>
            Air pollution has become one of the most pressing environmental issues affecting public health globally.
            Accurately estimating the Air Quality Index (AQI) is crucial for developing preventive health measures and
            urban planning strategies. Traditional AQI prediction models rely solely on numerical sensor data, such as
            PM2.5, PM10, NO<sub>2</sub>, SO<sub>2</sub>, and CO concentrations. However, these approaches struggle to
            capture broader contextual cues like weather conditions and environmental visuals <sup><a href="#ref1">[1]</a></sup>.
          </p>
          <p>
            Recent advancements in multimodal machine learning show promising potential in enhancing predictive accuracy
            by integrating complementary information from multiple modalities. In our proposed system, we combine numerical
            sensor data with visual data (sky images, urban visuals) using a deep neural model. This enables our model to
            infer air quality patterns not only from raw sensor metrics but also from the atmospheric and spatial patterns
            found in images <sup><a href="#ref2">[2]</a></sup>.
          </p>
          <p>
            By incorporating contrastive learning <sup><a href="#ref5">[5]</a></sup>, attention-based fusion <sup><a href="#ref4">[4]</a></sup>, and deep visual encoders <sup><a href="#ref3">[3]</a></sup>,
            our framework demonstrates superior AQI prediction capabilities. The approach is especially beneficial for urban and semi-urban regions
            with incomplete sensor coverage <sup><a href="#ref2">[2]</a></sup>.
          </p>
        </div>

        <!-- Numerical Embedding -->
        <div class="section dashed-box">
          <h2>NUMERICAL EMBEDDING AND GATED FUSION</h2>
          <p>
            The numerical branch is specifically designed to extract robust, high-level embeddings from structured environmental data.
            The input vector contains features such as PM2.5, PM10, temperature, humidity, CO, SO<sub>2</sub>, and NO<sub>2</sub>.
            These variables are passed through three dense layers with ReLU activation functions, along with dropout regularization to
            prevent overfitting and enhance generalization <sup><a href="#ref1">[1]</a></sup>.
          </p>
          <p>
            After numerical and visual features are aligned using attention, the representations are concatenated and passed through
            a gated fusion module. The core of this module is a sigmoid-activated dense layer that learns a soft gate vector. This gate
            controls how much each modality contributes to the final embedding <sup><a href="#ref4">[4]</a></sup>.
          </p>
          <p style="text-align: center;"><code>FusedOutput = Gate * Attended + (1 - Gate) * Combined</code></p>
          <p>
            This architecture ensures robustness in real-world conditions. For instance, if a sky image is taken in low light or
            weather-obscured conditions, the gate learns to favor numerical data. Conversely, if certain sensors are noisy or malfunctioning,
            image features are weighted more heavily <sup><a href="#ref2">[2]</a></sup>.
          </p>
          <p>
            Empirical results from our experiments demonstrate that this adaptive gating significantly improves performance compared
            to fixed fusion schemes <sup><a href="#ref1">[1]</a></sup>. It also improves the model's resilience to modality dropout, a common issue in multimodal systems.
          </p>
        </div>
      </div>

      <!-- Right Column -->
      <div class="column">
        <!-- Attention Box -->
        <div class="section dashed-box" style="margin-top: 0px;">
          <h2>CROSS-MODAL AND SELF-ATTENTION MECHANISMS</h2>
          <p>
            To effectively integrate heterogeneous features from numerical and visual inputs, our model employs a cross-modal
            attention mechanism. The numerical representation acts as the query, while image embeddings serve as keys and values.
            This allows the numerical features to attend to relevant visual semantics, such as haziness, cloud coverage, or sky tint
            <sup><a href="#ref2">[2]</a></sup>.
          </p>
          <p>
            After this initial alignment, we pass the fused representation into a multi-head self-attention (MHSA) block.
            This layer enables the model to model dependencies and interactions between different parts of the feature vector
            across both modalities. Self-attention proves especially useful in capturing global context, such as how temperature
            might interact with haze observed in the image <sup><a href="#ref4">[4]</a></sup>.
          </p>
          <p>
            The design draws inspiration from the Transformer architecture, but is adapted for compact multimodal regression tasks.
            Layer normalization and residual connections ensure training stability and convergence <sup><a href="#ref3">[3]</a></sup>.
          </p>
        </div>

        <!-- Image Grid -->
        <div class="figure">
          
          <div class="image-grid">
            <div>
              <img src="/AQI_image/11203.jpg" alt="Box 1">
              <div class="caption">1. Day 1</div>
            </div>
            <div>
              <img src="/AQI_image/frame_00009.jpg" alt="Box 2">
              <div class="caption">2. Day 2</div>
            </div>
            <div>
              <img src="/AQI_image/frame_00023.jpg" alt="Box 3">
              <div class="caption">3. Day 3</div>
            </div>
            <div>
              <img src="/AQI_image/156.jpg" alt="Box 4">
              <div class="caption">4. Night 1</div>
            </div>
            <div>
              <img src="/AQI_image/168.jpg" alt="Box 5">
              <div class="caption">5. Night 2</div>
            </div>
            <div>
              <img src="/AQI_image/11225.jpg" alt="Box 6">
              <div class="caption">6. Day 4</div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Conclusion -->
    <div class="section dashed-box">
      <h2>CONCLUSION</h2>
      <p>
        We present a novel multimodal AQI prediction model that leverages contrastive learning, attention-based fusion, and a
        gated decision mechanism to improve regression accuracy. By effectively integrating structured sensor data and unstructured
        visual information, our model outperforms traditional baselines and offers a flexible, real-time solution for environmental
        monitoring.This approach is especially beneficial in urban environments where data collection is sparse or heterogeneous. Our use of
        cross-modal attention ensures context-sensitive fusion, while the contrastive objective aligns embeddings during training.
        The gated fusion mechanism introduces dynamic adaptability based on data reliability.Future research will focus on enhancing interpretability, extending to multi-location AQI forecasting, and incorporating
        temporal dynamics through recurrent or transformer-based extensions. Our framework can also be adapted to other
        environmental monitoring tasks such as flood risk estimation or climate condition classification.
      </p>
     
    </div>
    <!-- References -->
    <div class="section dashed-box">
      <h2>REFERENCES</h2>
      <ol style="padding-left: 22px; line-height: 1.6; margin: 0;">
        <li>Ma, L., et al. (2022). <em>Deep Fusion for Multimodal Air Quality Prediction</em>. IEEE Transactions on Geoscience and Remote Sensing.</li>
        <li>Zhang, H., et al. (2021). <em>Image and Sensor Fusion for Urban Air Quality Forecasting</em>. Environmental Modelling & Software.</li>
        <li>Radford, A., et al. (2021). <em>Learning Transferable Visual Models From Natural Language Supervision</em>. In Proceedings of ICML.</li>
        <li>Lee, S., et al. (2023). <em>AirFormer: Attention-based Fusion for Air Quality Estimation</em>. AAAI Conference on Artificial Intelligence.</li>
        <li>Chen, T., et al. (2020). <em>A Simple Framework for Contrastive Learning of Visual Representations</em>. In Proceedings of ICML.</li>
      </ol>
    </div>
    
    
  </div>
</body>
</html>
