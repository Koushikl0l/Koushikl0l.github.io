<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Robust Fusion and Contrastive Learning for Multimodal AQI Regression</title>
  <style>
    body {
      font-family: "Times New Roman", Times, serif;
      font-size: 14px;
      margin: 0;
      padding: 0;
      background-color: #fff;
      color: #000;
    }
    .container {
      width: 90%;
      max-width: 1100px;
      margin: 40px auto;
    }
    .title {
      text-align: center;
      font-size: 22px;
      font-weight: bold;
    }
    .author {
      text-align: center;
      font-size: 14px;
      margin-bottom: 20px;
    }
    .section {
      margin-bottom: 30px;
    }
    .abstract-box {
      border: 1px dashed #999;
      padding: 15px;
      margin-bottom: 20px;
    }
    .keywords {
      margin-top: -10px;
      margin-bottom: 30px;
    }
    .columns {
      display: flex;
      gap: 20px;
    }
    .column {
      flex: 1;
    }
    .figure {
      text-align: center;
      font-size: 12px;
      margin-top: 10px;
      margin-bottom: 20px;
    }
    .image-grid {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 10px;
      margin: 20px 0;
    }
    .image-grid img {
      width: 100%;
      border: 1px solid #999;
      border-radius: 4px;
    }
    .caption {
      text-align: center;
      font-size: 11px;
      color: #444;
    }
    h2 {
      font-size: 16px;
      margin: 15px 0 5px;
    }
    .dashed-box {
      border: 1.5px dashed #666;
      padding: 10px;
      margin-bottom: 20px;
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="title">
      Robust Fusion and Contrastive Learning for Multimodal AQI Regression
    </div>
    <div class="author">
      FirstName LastName <br>
      Department of Computer Science, University Name <br>
      email@example.com
    </div>

    <div class="abstract-box">
      <strong>ABSTRACT</strong>
      <p>
        A multimodal approach for predicting Air Quality Index (AQI) by integrating numerical and image data.
        A precision-integrated numerical branch and an image encoder using EfficientNetB0 extract features.
        Cross-modal attention is utilized to enhance interaction between modalities, followed by multi-head self-attention
        and a gated fusion mechanism for robust integration. A contrastive loss function is incorporated during training
        to align numerical and image embeddings, leading to state-of-the-art performance in AQI regression from multiple data sources.
      </p>
    </div>

    <div class="keywords">
      <strong>KEYWORDS</strong>: Multimodal learning, AQI regression, contrastive loss, robust fusion
    </div>

    <div class="columns">
      <div class="column">
        <div class="section">
          <h2>1 INTRODUCTION</h2>
<p>
Air pollution has become one of the most pressing environmental issues affecting public health globally. 
Accurately estimating the Air Quality Index (AQI) is crucial for developing preventive health measures and 
urban planning strategies. Traditional AQI prediction models rely solely on numerical sensor data, such as 
PM2.5, PM10, NO<sub>2</sub>, SO<sub>2</sub>, and CO concentrations. However, these approaches struggle to 
capture broader contextual cues like weather conditions and environmental visuals.

Recent advancements in multimodal machine learning show promising potential in enhancing predictive accuracy 
by integrating complementary information from multiple modalities. In our proposed system, we combine numerical 
sensor data with visual data (sky images, urban visuals) using a deep neural model. This enables our model to 
infer air quality patterns not only from raw sensor metrics but also from the atmospheric and spatial patterns 
found in images.

By incorporating contrastive learning, attention-based fusion, and deep visual encoders, our framework demonstrates 
superior AQI prediction capabilities. The approach is especially beneficial for urban and semi-urban regions 
with incomplete sensor coverage.
</p>

        </div>

        <div class="section dashed-box">
          <h2>3 NUMERICAL EMBEDDING AND GATED FUSION</h2>
<p>
The numerical branch processes a 7-dimensional vector that includes environmental attributes like temperature, 
humidity, and gas concentrations. This vector is passed through three fully connected layers with ReLU activations 
and dropout regularization, allowing the model to extract a compact and noise-resistant embedding.

After alignment with the image embedding through attention, both are concatenated. To refine this fused representation, 
we introduce a gated fusion unit. This unit dynamically weighs the contribution of each modality using a sigmoid-based 
learned gating function. The formula is:
</p>
<p style="text-align:center;">
<code>fused = gate * attention_output + (1 - gate) * raw_concat</code>
</p>
<p>
This architecture allows the model to handle missing or noisy data more gracefully. For example, in the absence of 
useful image cues (e.g., night-time shots or obstructed views), the gate learns to emphasize numerical inputs.
Conversely, when sensor readings are inconsistent or corrupted, image features take priority. This adaptivity 
significantly improves real-world robustness.
</p>

        </div>
      </div>

      <div class="column">
        <div class="section dashed-box">
          <h2>2 CROSS-MODAL AND SELF-ATTENTION MECHANISMS</h2>
<p>
To effectively integrate heterogeneous features from numerical and visual inputs, our model employs a cross-modal 
attention mechanism. The numerical representation acts as the query, while image embeddings serve as keys and values. 
This allows the numerical features to attend to relevant visual semantics, such as haziness, cloud coverage, or sky tint.

After this initial alignment, we pass the fused representation into a multi-head self-attention (MHSA) block. 
This layer enables the model to model dependencies and interactions between different parts of the feature vector 
across both modalities. Self-attention proves especially useful in capturing global context, such as how temperature 
might interact with haze observed in the image.

The design draws inspiration from the Transformer architecture, but is adapted for compact multimodal regression tasks. 
Layer normalization and residual connections ensure training stability and convergence.
</p>

        </div>

        <div class="figure">
          <strong>Figure 1: Cross-Modal AQI Regression Architecture</strong>
          <div class="image-grid">
            <div>
              <img src="images/box1_numerical_branch.png" alt="Box 1">
              <div class="caption">1. Numerical Branch</div>
            </div>
            <div>
              <img src="images/box2_image_branch.png" alt="Box 2">
              <div class="caption">2. Image Branch</div>
            </div>
            <div>
              <img src="images/box3_attention.png" alt="Box 3">
              <div class="caption">3. Cross-Modal Attention</div>
            </div>
            <div>
              <img src="images/box4_self_attention.png" alt="Box 4">
              <div class="caption">4. Self-Attention</div>
            </div>
            <div>
              <img src="images/box5_gated_fusion.png" alt="Box 5">
              <div class="caption">5. Gated Fusion</div>
            </div>
            <div>
              <img src="images/box6_output.png" alt="Box 6">
              <div class="caption">6. Final Output</div>
            </div>
          </div>
        </div>
      </div>
    </div>

  </div>
</body>
</html>
