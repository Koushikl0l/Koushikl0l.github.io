<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CheXNeXt: Deep learning for chest radiograph diagnosis</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Product+Sans:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Product Sans', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: white;
        }

        /* Header with geometric pattern */
        .header {
            background: #a41d13f8;
            background-image: 
                linear-gradient(30deg, transparent 40%, rgba(255, 255, 255, 0.1) 40%, rgba(107, 23, 23, 0.1) 60%, transparent 60%),
                linear-gradient(150deg, transparent 40%, rgba(255,255,255,0.05) 40%, rgba(204, 36, 36, 0.05) 60%, transparent 60%);
            background-size: 60px 60px, 80px 80px;
            color: white;
            padding: 25px 20px;
            text-align: left;
            position: relative;
        }

        .header-content {
            max-width: 1200px;
            margin: 0 auto;
        }

        .stanford-label {
            font-size: 18px;
            font-weight: 400;
            margin-bottom: 15px;
            opacity: 0.9;
        }

        .main-title {
            font-size: 48px;
            font-weight: 300;
            margin-bottom: 20px;
            line-height: 1.2;
        }

        .main-title strong {
            font-weight: 600;
        }

        .authors {
            font-size: 16px;
            line-height: 1.5;
            opacity: 0.95;
            max-width: 900px;
        }

        .authors sup {
            font-size: 12px;
        }

        /* Main content */
        .container {
            max-width: 1280px;
            margin: 0 auto;
            padding: 0 20px;
        }

        .content-section {
            background: white;
            margin: 0;
            padding: 50px 40px;
        }

        .content-section:nth-child(even) {
            background: #fafafa;
        }

        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 60px;
            align-items: center;
            min-height: 500px;
        }

        .section-title {
            font-size: 30px;
            font-weight: 300;
            color: #333;
            margin-bottom: 20px;
            line-height: 1.3;
            text-align: left;
        }

        .section-text {
            font-size: 18px;
            line-height: 1.6;
            color: #555;
            margin-bottom: 20px;
            text-align: justify;
            text-align-last: left;
        }

        /* Video embed styling */
        .video-container {
            position: relative;
            width: 100%;
            max-width: 500px;
        }

        .video-embed {
            width: 100%;
            height: 280px;
            border-radius: 8px;
            overflow: hidden;
            background: #000;
            position: relative;
        }

        .video-embed img {
            width: 100%;
            height: 100%;
            object-fit: contain;
            display: block;
        }

        /* Intro image (replaces video container) */
        .intro-image {
            width: 100%;
            max-width: 500px;
            height: 500px;
            border-radius: 8px;
            display: block;
            object-fit: cover;
        }

        /* Training section image */
        .training-image {
            width: 100%;
            max-width: 500px;
            height: auto;
            border-radius: 8px;
            display: block;
            object-fit: cover;
        }

        /* ROC/Results image */
        .results-image {
            width: 100%;
            max-width: 500px;
            height: 500px;
            border-radius: 8px;
            display: block;
            object-fit: contain;
        }

        .video-placeholder {
            width: 100%;
            height: 100%;
            background: linear-gradient(135deg, #2a2a2a 0%, #1a1a1a 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 16px;
        }

        /* X-ray images grid */
        .xray-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 15px;
        }

        .xray-pair {
            display: flex;
            gap: 15px;
        }

        .xray-image {
            width: 100%;
            height: 200px;
            background: #e9ecef;
            border-radius: 8px;
            position: relative;
            overflow: hidden;
        }

        .xray-original {
            background: linear-gradient(45deg, #f8f9fa 25%, #e9ecef 25%, #e9ecef 50%, #f8f9fa 50%, #f8f9fa 75%, #e9ecef 75%);
            background-size: 20px 20px;
        }

        .xray-heatmap {
            background: radial-gradient(circle at 60% 40%, rgba(255, 100, 150, 0.8) 0%, rgba(150, 50, 200, 0.6) 40%, rgba(50, 50, 150, 0.4) 70%, transparent 100%),
                        radial-gradient(circle at 40% 70%, rgba(255, 150, 50, 0.7) 0%, rgba(200, 100, 100, 0.5) 40%, transparent 70%),
                        linear-gradient(135deg, #2a2a2a 0%, #1a1a1a 100%);
        }

        /* ROC curves grid */
        .roc-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 20px;
            margin: 30px 0;
        }

        .roc-chart {
            background: white;
            border: 1px solid #dee2e6;
            border-radius: 6px;
            padding: 15px;
            text-align: center;
        }

        .roc-title {
            font-size: 14px;
            font-weight: 600;
            margin-bottom: 10px;
            color: #333;
        }

        .roc-plot {
            width: 100%;
            height: 120px;
            background: #f8f9fa;
            border-radius: 4px;
            position: relative;
            overflow: hidden;
        }

        /* Simulated ROC curve */
        .roc-plot::before {
            content: '';
            position: absolute;
            bottom: 10px;
            left: 10px;
            right: 10px;
            top: 10px;
            border-left: 2px solid #ccc;
            border-bottom: 2px solid #ccc;
        }

        .roc-plot::after {
            content: '';
            position: absolute;
            bottom: 10px;
            left: 10px;
            width: calc(100% - 20px);
            height: 3px;
            background: linear-gradient(45deg, #28a745 0%, #17a2b8 50%, #6f42c1 100%);
            border-radius: 2px;
            clip-path: polygon(0% 100%, 20% 80%, 40% 60%, 60% 40%, 80% 20%, 100% 0%);
        }

        /* Performance metrics */
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 20px;
        }

        .metric-chart {
            background: white;
            border: 1px solid #dee2e6;
            border-radius: 6px;
            padding: 20px;
        }

        .metric-title {
            font-size: 18px;
            font-weight: 600;
            margin-bottom: 15px;
            color: #333;
        }

        .metric-plots {
            display: grid;
            grid-template-columns: repeat(6, 1fr);
            gap: 10px;
        }

        .metric-plot {
            text-align: center;
        }

        .metric-plot-title {
            font-size: 12px;
            margin-bottom: 5px;
            color: #666;
        }

        .plot-area {
            width: 100%;
            height: 80px;
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 4px;
            position: relative;
            overflow: hidden;
        }

        /* Simulated scatter plots */
        .plot-area::before {
            content: '';
            position: absolute;
            top: 20%;
            left: 50%;
            width: 4px;
            height: 4px;
            background: #28a745;
            border-radius: 50%;
            box-shadow: 
                -10px 5px 0 #17a2b8,
                5px -8px 0 #dc3545,
                -15px 15px 0 #ffc107,
                10px 10px 0 #6f42c1,
                -5px -5px 0 #28a745,
                15px -15px 0 #fd7e14;
        }

        /* Partnership section */
        .partnership-section {
            background: transparent;
            text-align: center;
        }

        .partnership-content {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 40px;
            align-items: center;
        }

        .partnership-images {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }

        .phone-demo {
            width: 100%;
            max-width: 500px;
            height: 400px;
            background: white;
            border-radius: 5px;
            position: relative;
            box-shadow: 0 8px 24px rgba(0,0,0,0.15);
            overflow: hidden;
            margin: 0 auto;
        }

        .phone-demo img {
            width: 100%;
            height: 100%;
            object-fit: contain;
            display: block;
            padding: 20px;
        }

        .partner-btn {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 14px 28px;
            background: #8C1515;
            color: white;
            text-decoration: none;
            font-weight: 600;
            font-size: 14px;
            letter-spacing: 0.5px;
            border-radius: 4px;
            transition: all 0.3s ease;
            margin-top: 20px;
            border: 2px solid #8C1515;
            box-shadow: 0 2px 4px rgba(140, 21, 21, 0.2);
        }

        .partner-btn::before {
            content: "📄";
            font-size: 16px;
        }

        .partner-btn:hover {
            background: #B1040E;
            border-color: #B1040E;
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(140, 21, 21, 0.3);
        }

        .partner-btn:active {
            transform: translateY(0);
            box-shadow: 0 2px 4px rgba(140, 21, 21, 0.2);
        }

        /* Copyright section */
        .copyright-section {
            background: #f8f9fa;
            padding: 25px 0;
            text-align: center;
            border-top: 1px solid #e9ecef;
        }

        .copyright-content {
            max-width: 1280px;
            margin: 0 auto;
            padding: 0 20px;
        }

        .copyright-text {
            color: #6c757d;
            font-size: 14px;
            margin: 0;
        }

        /* Responsive design */
        @media (max-width: 1200px) {
            .container {
                max-width: 95%;
                padding: 0 15px;
            }
        }

        @media (max-width: 968px) {
            .two-column,
            .partnership-content {
                grid-template-columns: 1fr;
                gap: 40px;
            }
            
            .main-title {
                font-size: 36px;
            }

            .section-title {
                font-size: 26px;
            }

            .intro-image,
            .training-image,
            .results-image {
                max-width: 100%;
                height: auto;
                margin: 0 auto;
            }

            .phone-demo {
                max-width: 100%;
                height: 300px;
            }
        }

        @media (max-width: 768px) {
            .header {
                padding: 30px 15px;
            }
            
            .main-title {
                font-size: 28px;
            }

            .section-title {
                font-size: 24px;
            }

            .section-text {
                font-size: 15px;
                line-height: 1.5;
            }
            
            .content-section {
                padding: 40px 20px;
            }

            .two-column {
                gap: 30px;
            }

            .partner-btn {
                padding: 12px 24px;
                font-size: 13px;
            }
        }

        @media (max-width: 480px) {
            .header {
                padding: 25px 10px;
            }

            .main-title {
                font-size: 24px;
            }

            .section-title {
                font-size: 22px;
            }

            .section-text {
                font-size: 14px;
            }

            .content-section {
                padding: 30px 15px;
            }

            .container {
                padding: 0 10px;
            }

            .partner-btn {
                padding: 10px 20px;
                font-size: 12px;
            }

            .phone-demo {
                height: 250px;
            }
        }
    </style>
</head>
<body>
    <!-- Header Section -->
    <header class="header">
        <div class="header-content">
        
            <h1 class="main-title">
                <strong>AQFusionNet</strong> Robust Multimodal Deep Learning
                for Air Quality Index Prediction through
                Atmospheric Imagery and Environmental Sensor
                Integration
            </h1>
            <div class="authors">
                Koushik Ahmed Kushal <sup>*</sup>, Abdullah Al Mamun
            </div>
        </div>
    </header>

    <div class="container">
        <!-- Introduction Section -->
        <section class="content-section">
            <div class="two-column">
                <div>
                    <h2 class="section-title">We developed AQFusionNet, a deep learning algorithm to predict the Air Quality Index (AQI) from atmospheric imagery and environmental sensor data.</h2>
                    <p class="section-text">
                        Air quality assessment is critical for monitoring and mitigating health risks from pollution-related diseases, which impact millions of people worldwide each year. This process often relies on sensor networks and manual interpretation of environmental data, which can be time-consuming, prone to errors, and limited in regions where reliable sensors or expertise are unavailable.
                    </p>
                </div>
                <img class="intro-image" src="AQI_image/model.jpg" alt="AQFusionNet heatmap visualization">
            </div>
        </section>

        <!-- Training Section -->
        <section class="content-section">
            <div class="two-column">
                <img class="training-image" src="AQI_image/input.png" alt="AQFusionNet training data visualization">
                <div>
                    <h2 class="section-title">AQFusionNet is trained to predict air quality levels by combining cityscape or sky images with available sensor data.</h2>
                    <p class="section-text">
                        AQFusionNet is trained on multimodal air quality datasets, combining thousands of environmental sensor readings with corresponding atmospheric images. Each sample is paired with ground-truth AQI values derived from standardized monitoring stations.
                    </p>
                    <p class="section-text">
                        AQFusionNet is trained on a multimodal dataset that combines atmospheric images with corresponding air quality sensor readings. The dataset includes thousands of samples where sky or city images are paired with pollutant measurements (such as PM2.5, PM10, NO₂, and CO) and ground-truth AQI values from monitoring stations.
                    </p>
                    <p class="section-text">
                        This multimodal setup enables the model to learn visual cues of pollution from images while aligning them with numerical sensor data for more accurate AQI prediction."
                    </p>
                </div>
            </div>
        </section>

        <!-- Validation Section -->
        <section class="content-section">
            <div class="two-column">
                <div>
                    <h2 class="section-title">We compared AQFusionNet's performance on different baseline with test set.</h2>
                    <p class="section-text">
                        We evaluated AQFusionNet with different CNN backbones against baseline models. On the test set with 1237 samples, the EfficientNet-B0 backbone achieved the best performance with an RMSE of 7.70 and accuracy of 92.02%, outperforming MobileNetV2 (8.89, 90.45%) and ResNet18 (8.67, 90.95%). Its 95% CI of [6.14, 9.20] was also narrower than MobileNetV2 [7.72, 9.96] and ResNet18 [7.13, 9.84], showing greater robustness.
                    </p>
                    <p class="section-text">
                        ROC analysis further confirmed this advantage, with a macro-average AUC of 0.95. Critical AQI categories such as Unhealthy (0.97), Very Unhealthy (0.99), and Hazardous (0.99) achieved near-perfect separability, while Good (0.89) and Unhealthy for Sensitive Groups (0.89) also maintained strong discrimination.
                    </p>
                    <p class="section-text">
                        Overall, AQFusionNet delivers lower errors, tighter confidence intervals, and stronger classification performance than other backbone variants and baselines, establishing it as a reliable solution for multimodal AQI prediction.
                    </p>
                </div>
                <img class="results-image" src="AQI_image/r.png" alt="AQFusionNet performance results and ROC analysis">
            </div>
        </section>



        <!-- Partnership Section -->
        <section class="content-section partnership-section">
            <div class="partnership-content">
                <div class="partnership-images">
                    <div class="phone-demo">
                        <img src="AQI_image/check.png" alt="AQFusionNet validation results">
                    </div>
                </div>
                <div>
                    <h2 class="section-title">We are currently looking for research partnerships with environmental and public health agencies that are interested in working with us to validate this technology.</h2>
                    <p class="section-text">
                        We hope that this system may have the potential to improve air quality monitoring and increase access to reliable pollution insights globally.Towards this goal, the future of this research will depend on obtaining access to more diverse datasets for training and improving the model, as well as testing it across new geographic regions and environmental conditions. This additional data will help improve the accuracy and robustness of the model, making it more safe and effective.
                    </p>
                    <a href="https://github.com/Koushikl0l/Koushikl0l.github.io/blob/main/Multimodal_Deep_Learning_for_Air_Quality_Index_Prediction_Using_Atmospheric_Images_and_Sensor_Data_IEEE-5.pdf" class="partner-btn">Read AQFusionNet paper</a>
                </div>
            </div>
        </section>
    </div>

    <!-- Copyright Section -->
    <footer class="copyright-section">
        <div class="copyright-content">
            <p class="copyright-text">
                © 2024 AQFusionNet. All rights reserved. | 
                Developed by Koushik Ahmed Kushal <sup>*</sup> & Abdullah Al Mamun | 
                IEEE Publication (Manuscript in preparation)
            </p>
        </div>
    </footer>
</body>
</html>
